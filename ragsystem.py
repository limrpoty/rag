import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from src.config import RAGConfig
from src.memory import ConversationMemory
from src.loaders import DocumentLoader, WebScraper
from src.processing import TextChunker
from src.llm import OllamaManager

class RAGSystem:
    """Sistema RAG completo com busca vetorial e geraÃ§Ã£o de respostas usando Ollama"""

    def __init__(self, model_name: str = RAGConfig.OLLAMA_MODEL, memory_turns: int = 3):
        """
        Inicializa o sistema RAG com memÃ³ria conversacional

        Args:
            model_name: Nome do modelo Ollama a usar
            memory_turns: NÃºmero de turnos de conversa a manter na memÃ³ria
        """
        print("ğŸ”§ Inicializando Sistema RAG (100% Open Source)...")

        # Verifica se Ollama estÃ¡ rodando
        if not OllamaManager.check_ollama_running():
            raise Exception(
                "âŒ Ollama nÃ£o estÃ¡ rodando!\n"
                "Execute no Colab:\n"
                "!curl -fsSL https://ollama.com/install.sh | sh\n"
                "!nohup ollama serve > ollama.log 2>&1 &\n"
                "!sleep 5"
            )

        print("âœ… Ollama estÃ¡ rodando!")

        # Verifica se modelo estÃ¡ disponÃ­vel, senÃ£o baixa
        if not OllamaManager.check_model_available(model_name):
            print(f"âš ï¸  Modelo {model_name} nÃ£o encontrado localmente.")
            OllamaManager.pull_model(model_name)
        else:
            print(f"âœ… Modelo {model_name} disponÃ­vel!")

        self.model_name = model_name

        # ğŸ†• CRÃTICO: Inicializa memÃ³ria conversacional
        self.memory = ConversationMemory(max_turns=memory_turns)
        print(f"ğŸ§  MemÃ³ria conversacional ativada ({memory_turns} turnos)")

        # Inicializa modelo de embeddings (roda localmente, sem custo)
        print("ğŸ“¥ Carregando modelo de embeddings...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=RAGConfig.EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        # Inicializa componentes
        self.chunker = TextChunker()
        self.vectorstore = None
        self.documents = []

        print("âœ… Sistema RAG inicializado com sucesso!\n")

    def add_document(self, file_path: str) -> None:
        """Adiciona documento ao sistema"""
        try:
            print(f"ğŸ“„ Processando arquivo: {file_path}")

            # Carrega documento
            text = DocumentLoader.load_file(file_path)

            # Cria chunks com metadata
            metadata = {
                'source': file_path,
                'source_type': 'file',
                'filename': os.path.basename(file_path)
            }
            chunks = self.chunker.chunk_text(text, metadata)

            self.documents.extend(chunks)
            print(f"âœ… Arquivo processado: {len(chunks)} chunks criados\n")

        except Exception as e:
            print(f"âŒ Erro ao processar arquivo: {str(e)}\n")
            raise

    def add_url(self, url: str) -> None:
        """Adiciona conteÃºdo de URL ao sistema"""
        try:
            print(f"ğŸŒ Fazendo scraping da URL: {url}")

            # Faz scraping
            text = WebScraper.scrape_url(url)

            # Cria chunks com metadata
            metadata = {
                'source': url,
                'source_type': 'url'
            }
            chunks = self.chunker.chunk_text(text, metadata)

            self.documents.extend(chunks)
            print(f"âœ… URL processada: {len(chunks)} chunks criados\n")

        except Exception as e:
            print(f"âŒ Erro ao processar URL: {str(e)}\n")
            raise

    def build_vectorstore(self) -> None:
        """ConstrÃ³i o vector store a partir dos documentos adicionados"""
        try:
            if not self.documents:
                raise ValueError("Nenhum documento foi adicionado ao sistema")

            print(f"ğŸ”¨ Construindo vector store com {len(self.documents)} chunks...")

            # Cria vector store
            self.vectorstore = Chroma.from_documents(
                documents=self.documents,
                embedding=self.embeddings,
                persist_directory=RAGConfig.PERSIST_DIRECTORY
            )

            print("âœ… Vector store construÃ­do com sucesso!\n")

        except Exception as e:
            print(f"âŒ Erro ao construir vector store: {str(e)}\n")
            raise

    def retrieve_context(self, query: str, top_k: int = RAGConfig.TOP_K_RESULTS) -> List[Document]:
        """Recupera chunks mais relevantes para a query"""
        try:
            if self.vectorstore is None:
                raise ValueError("Vector store nÃ£o foi construÃ­do. Execute build_vectorstore() primeiro.")

            # Busca por similaridade
            results = self.vectorstore.similarity_search(query, k=top_k)

            return results

        except Exception as e:
            print(f"âŒ Erro na busca: {str(e)}")
            raise

    def generate_answer(self, query: str, context_docs: List[Document]) -> str:
        """Gera resposta usando Ollama baseado no contexto recuperado E histÃ³rico de conversa"""
        try:
            # Formata contexto dos documentos
            context = "\n\n---\n\n".join([
                f"[Fonte: {doc.metadata.get('source', 'Desconhecida')}]\n{doc.page_content}"
                for doc in context_docs
            ])

            # ğŸ†• ObtÃ©m histÃ³rico de conversa
            conversation_history = self.memory.get_formatted_history()

            # ğŸ†• NOVO: Prompt melhorado com detecÃ§Ã£o de mudanÃ§a de contexto
            user_prompt = f"""=== HISTÃ“RICO DA CONVERSA ===
{conversation_history}

=== CONTEXTO DOS DOCUMENTOS ===
{context}

=== INSTRUÃ‡Ã•ES CRÃTICAS ===
1. **DETECÃ‡ÃƒO DE MUDANÃ‡A DE ASSUNTO:**
   - Se a pergunta atual NÃƒO se relaciona com o histÃ³rico (ex: muda completamente de tema), IGNORE o histÃ³rico e responda APENAS com base nos documentos.
   - Exemplo: Se o histÃ³rico fala sobre "UBS" e a pergunta Ã© sobre "casa de cachorro", a pergunta NÃƒO tem relaÃ§Ã£o, entÃ£o ignore o histÃ³rico.

2. **USO DO HISTÃ“RICO:**
   - Use o histÃ³rico APENAS quando a pergunta se refere explicitamente a algo mencionado antes (palavras como "isso", "elas", "aquilo", "o que vocÃª disse").
   - Exemplo: "Quais sÃ£o os horÃ¡rios delas?" â†’ "delas" se refere a algo do histÃ³rico.

3. **PRIORIDADE:**
   - SEMPRE responda com base nos DOCUMENTOS, nÃ£o em inferÃªncias.
   - Se a informaÃ§Ã£o NÃƒO estÃ¡ nos documentos, diga claramente: "NÃ£o encontrei essa informaÃ§Ã£o nos documentos."
   - NUNCA invente informaÃ§Ãµes ou repita respostas anteriores se nÃ£o forem relevantes.

4. **CLAREZA:**
   - Seja direto e conciso.
   - NÃ£o repita informaÃ§Ãµes jÃ¡ ditas a menos que seja solicitado.

=== PERGUNTA ATUAL ===
{query}

Responda de forma objetiva baseando-se APENAS nas informaÃ§Ãµes dos documentos."""

            # Chama Ollama
            answer = OllamaManager.generate_response(
                model=self.model_name,
                prompt=user_prompt,
                system_prompt=RAGConfig.SYSTEM_PROMPT,
                temperature=0.3  # Baixa temperatura para respostas mais precisas
            )

            # ğŸ†• Adiciona interaÃ§Ã£o Ã  memÃ³ria
            self.memory.add_interaction(query, answer)

            return answer

        except Exception as e:
            return f"Erro ao gerar resposta: {str(e)}"

    def clear_memory(self) -> None:
        """Limpa o histÃ³rico de conversas"""
        self.memory.clear()

    def show_memory(self) -> None:
        """Exibe o histÃ³rico atual de conversas"""
        print("\n" + "="*70)
        print("ğŸ§  MEMÃ“RIA CONVERSACIONAL")
        print("="*70)
        print(f"Turnos armazenados: {self.memory.get_turn_count()}/{self.memory.max_turns}")
        print("\n" + self.memory.get_formatted_history())
        print("="*70 + "\n")

    def is_query_related_to_history(self, query: str) -> bool:
        """
        Verifica se a pergunta se relaciona com o histÃ³rico recente

        Args:
            query: Pergunta atual

        Returns:
            True se relacionada, False caso contrÃ¡rio
        """
        if not self.memory.history:
            return False

        # Palavras que indicam referÃªncia ao histÃ³rico
        reference_words = [
            'isso', 'aquilo', 'elas', 'eles', 'dela', 'dele', 'delas', 'deles',
            'anterior', 'antes', 'vocÃª disse', 'mencionou', 'falou', 'citou'
        ]

        query_lower = query.lower()

        # Se a pergunta contÃ©m palavras de referÃªncia, Ã© relacionada
        if any(word in query_lower for word in reference_words):
            return True

        # Se a pergunta tem mais de 10 palavras e nÃ£o tem referÃªncias, provavelmente Ã© nova
        if len(query.split()) > 10:
            return False

        # Para perguntas curtas, assume que pode ser relacionada
        return True

    def query(self, question: str, show_context: bool = False, auto_clear_memory: bool = False) -> str:
        """
        MÃ©todo principal: faz pergunta e retorna resposta

        Args:
            question: Pergunta do usuÃ¡rio
            show_context: Se True, mostra o contexto recuperado
            auto_clear_memory: Se True, limpa memÃ³ria ao detectar mudanÃ§a de assunto
        """
        try:
            print(f"\nâ“ Pergunta: {question}\n")

            # ğŸ†• NOVO: Detecta se Ã© uma mudanÃ§a de assunto
            if auto_clear_memory and not self.is_query_related_to_history(question):
                if self.memory.get_turn_count() > 0:
                    print("ğŸ”„ MudanÃ§a de assunto detectada. Limpando memÃ³ria anterior...\n")
                    self.memory.clear()

            # Recupera contexto
            print("ğŸ” Buscando informaÃ§Ãµes relevantes...")
            context_docs = self.retrieve_context(question)

            if show_context:
                print("\nğŸ“š Contexto recuperado:")
                for i, doc in enumerate(context_docs, 1):
                    print(f"\n--- Chunk {i} ---")
                    print(f"Fonte: {doc.metadata.get('source', 'Desconhecida')}")
                    print(f"ConteÃºdo: {doc.page_content[:200]}...")

            # Gera resposta
            print(f"\nğŸ’­ Gerando resposta com {self.model_name}...")
            answer = self.generate_answer(question, context_docs)

            print("\nâœ… Resposta gerada!\n")
            return answer

        except Exception as e:
            error_msg = f"Erro ao processar pergunta: {str(e)}"
            print(f"\nâŒ {error_msg}\n")
            return error_msg